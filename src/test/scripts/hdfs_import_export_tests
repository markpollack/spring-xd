#!/bin/bash

. xdapi

PWD=`pwd`
# Base path in HDFS
NOW=`date "+%H-%M-%S"`
BASE_PATH="/xdtest/$NOW/"
TEST_DIR='/tmp/xdtest/batchhdfs'

wait_for_server

echo 'Hadoop environment and classpath:'
set | grep HADOOP
hadoop classpath

set -e

echo -e "\nRunning batch HDFS import tests"
echo "HDFS base path is $BASE_PATH"

echo -e "\n\n*** Test 1. Load single CSV file, no rollover\n"

create_job csvjob "filehdfs --names=col1,col2,col3 --basePath=$BASE_PATH --filename=blah --suffix=csv"
create_stream csvstream "file --ref=true --dir=$PWD/csv --pattern=data.csv > queue:job:csvjob"

sleep 10

blah_size=`hdfs_size "$BASE_PATH/blah-0.csv"`
destroy_stream csvstream
destroy_job csvjob
echo "Checking size of file blah-0.csv"
assert_equals 6358 $blah_size || exit $?

echo -e "\n\n*** Test 2. Import duplicate copies of a CSV data file\n"

if [[ ! -f $TEST_DIR/csv/data1.csv ]]
then
  mkdir -p $TEST_DIR/csv
  for i in {1..4}
  do
    echo "Copying csv file $i to $TEST_DIR/csv"
    cp $PWD/csv/data.csv "$TEST_DIR/csv/data$i.csv"
  done
fi

create_job csvjob2 "filehdfs --names=col1,col2,col3 --rollover=5000 --basePath=$BASE_PATH --suffix=csv"
create_stream csvstream2 "file --ref=true --dir=$TEST_DIR/csv --pattern=*.csv > queue:job:csvjob2"

echo -e "\nWaiting for jobs to finish..."
sleep 25;

job2_size=`hdfs_size "$BASE_PATH/csvjob2*"`

destroy_stream csvstream2
destroy_job    csvjob2

echo "Checking size of HDFS results matches imports from $TEST_DIR/csv"
assert_equals 25432 $job2_size

echo -e "\n\n*** Test 3. Import files using hdfs sink\n"

create_stream csvstream3 "file --dir=$TEST_DIR/csv --pattern=data1.csv | hdfs --directory=$BASE_PATH"
sleep 10
undeploy_stream csvstream3
deploy_stream csvstream3
sleep 10
destroy_stream csvstream3

data_size=`hdfs_size "$BASE_PATH/csvstream3*"`
assert_equals 16966 $data_size || exit $?

if [[ ! -f $XD_HOME/lib/sqlite-jdbc-3.7.2.jar ]]
then
  echo 'sqlite jar is missing from XD_HOME/lib. Skipping JDBC tests'
  exit 0
fi

echo -e "\n\n Test 4. Export file from hdfs to JDBC"

if [[ -f $TEST_DIR/hdfsjdbc.db ]]
then
  rm $TEST_DIR/hdfsjdbc.db &> /dev/null
fi

mkdir -p $TEST_DIR

sqlite3 $TEST_DIR/hdfsjdbc.db 'create table blah (col1 varchar, col2 varchar, col3 varchar)'

create_job csvjob4 "hdfsjdbc --driverClass=org.sqlite.JDBC --url=jdbc:sqlite:$TEST_DIR/hdfsjdbc.db --resources=${BASE_PATH}blah-0.csv --columns=col1,col2,col3 --tableName=blah"
launch_job csvjob4
sleep 5
rows=`sqlite3 $TEST_DIR/hdfsjdbc.db 'select count(*) from blah'`
destroy_job csvjob4
echo "Checking row count in database table matches import from hdfs..."
assert_equals 292 $rows

echo -e "\n\n Test 5. Import back to hdfs from JDBC"

create_job csvjob5 "jdbchdfs --driverClass=org.sqlite.JDBC --url=jdbc:sqlite:$TEST_DIR/hdfsjdbc.db --basePath=$BASE_PATH --filename=jdbcblah --suffix=csv --columns=col1,col2,col3 --tableName=blah"
launch_job csvjob5
sleep 15

blah_size=`hdfs_size "$BASE_PATH/jdbcblah-0.csv"`
destroy_job csvjob5
echo "Checking size of file jdbcblah-0.csv"
assert_equals 6358 $blah_size

echo -e "\n\n Test 6. Import back to hdfs from JDBC using SQL"

create_job csvjob6 "jdbchdfs --driverClass=org.sqlite.JDBC --url=jdbc:sqlite:$TEST_DIR/hdfsjdbc.db --basePath=$BASE_PATH --filename=jdbcblahsql --suffix=csv --sql='select col1,col2,col3 from blah' --columns=col1,col2,col3"
launch_job csvjob6
sleep 15

blah_size=`hdfs_size "$BASE_PATH/jdbcblahsql-0.csv"`
destroy_job csvjob6
echo "Checking size of file jdbcblahsql-0.csv"
assert_equals 6358 $blah_size


echo "All good :-) !!"

